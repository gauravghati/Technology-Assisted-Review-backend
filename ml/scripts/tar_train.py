# -*- coding: utf-8 -*-
"""tar_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aHx4WttFfxtp6sjGNNSu7gfwNeK7EpFF
"""

# pip install pytorch-pretrained-bert

import nltk
import pandas as pd
import warnings
import string
import numpy as np
import re
import nltk
import os
from nltk.corpus import stopwords
from tqdm import tqdm#, trange
import gc
from keras.preprocessing.sequence import pad_sequences
# from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Concatenate
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.initializers import Constant
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Dropout, SpatialDropout1D, Bidirectional,  GaussianNoise
from keras.layers.pooling import GlobalMaxPooling1D, GlobalAveragePooling1D
from keras import regularizers
from keras.models import load_model
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import ModelCheckpoint
from sklearn.utils import resample
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import accuracy_score
# from keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
warnings.filterwarnings('ignore')

df = pd.read_csv('./tokenized_data_final.csv')
df

def sliceDocsFromDataset(df, size):
  class1 = df.loc[df['Class Index'] == 1].head(size)
  class2 = df.loc[df['Class Index'] == 2].head(size)
  class3 = df.loc[df['Class Index'] == 3].head(size)
  class4 = df.loc[df['Class Index'] == 4].head(size)

  df = df.drop(df.index[list(class1.index.values) + list(class2.index.values) + list(class3.index.values) + list(class4.index.values)])

  train = pd.concat([class1, class2, class3, class4])
  train.reset_index(drop=True, inplace=True)
  train = train.sample(frac=1).reset_index(drop=True)
  print("train set class counts:\n", train['Class Index'].value_counts())
  print("\n\ntrain set:\n", train.head())

  df.reset_index(drop=True, inplace=True)
  print("\n\nremaining dataset class counts:\n", df['Class Index'].value_counts())

  return df, train

# padding + divide train and test

def deepLearningPrep(train, test=-1):
  X = np.zeros((train.shape[0],MAXLEN),dtype=np.int)

  for i,ids in tqdm(enumerate(list(train['Tokenized']))):

      input_ids = [int(i) for i in ids.split()[:MAXLEN]]
      inp_len = len(input_ids)
      X[i,:inp_len] = np.array(input_ids)
      
  Y = pd.get_dummies(train['Class Index']).values

  if(type(test) is not int):
    X_test = np.zeros((test.shape[0],MAXLEN),dtype=np.int)

    for i,ids in tqdm(enumerate(list(test['Tokenized']))):

        input_ids = [int(i) for i in ids.split()[:MAXLEN]]
        inp_len = len(input_ids)
        X_test[i,:inp_len] = np.array(input_ids)

    Y_test = pd.get_dummies(test['Class Index']).values
    return X, Y, X_test, Y_test
  else:
    return X,Y

BERT_MODEL = 'bert-base-uncased'
CASED = 'uncased' in BERT_MODEL
MAXLEN = 600

os.system('pip install --no-index --find-links="../input/pytorchpretrainedbert/" pytorch_pretrained_bert')

from pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_bert.modeling import BertModel

BERT_FP = './bert-base-uncased'

def get_bert_embed_matrix():
    bert = BertModel.from_pretrained(BERT_FP)
    bert_embeddings = list(bert.children())[0]
    bert_word_embeddings = list(bert_embeddings.children())[0]
    mat = bert_word_embeddings.weight.data.numpy()
    return mat

embedding_matrix = get_bert_embed_matrix()

bert = BertModel.from_pretrained(BERT_FP)
bert.eval()
tokenizer = BertTokenizer(vocab_file='./bert-based-uncased.txt')

# define model

import keras
from keras import regularizers

lstm_out = 256
model2 = Sequential()
model2.add(Embedding(*embedding_matrix.shape, weights=[embedding_matrix], input_length=MAXLEN, trainable=False))
model2.add(Bidirectional(LSTM(lstm_out, dropout = 0.5, recurrent_dropout=0.6, return_sequences = True), input_shape=(150, 300)))
model2.add(Dropout(0.4))

model2.add(GlobalMaxPooling1D())

model2.add(Dropout(0.4))
model2.add(Dense(4,activation='sigmoid'))

optimizer = Adam(lr=0.001)
model2.compile(loss = 'categorical_crossentropy',  optimizer = optimizer,metrics = ['accuracy'])
print(model2.summary())

df, train = sliceDocsFromDataset(df, 12)
X_train_AG, Y_train_AG = deepLearningPrep(train)


history = model2.fit(X_train_AG, Y_train_AG, epochs = 20,batch_size = 4, validation_split=0.2, verbose = 1)

df, test = sliceDocsFromDataset(df, 50)
X_test, Y_test = deepLearningPrep(test)

#check the current accuracy for those incoming docs
predict_x=model2.predict(X_test)
classes_x=np.argmax(predict_x,axis=1)
df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':classes_x})
df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))
print("confusion matrix",confusion_matrix(df_test.true, df_test.pred))
print(classification_report(df_test.true, df_test.pred, digits = 4))

model2.save("trained_model.h5")
model2 = load_model('trained_model.h5')


def train(model, train_data, validation_split = 0): #put validation split as 0.2 if not passing a single document
  X_train, y_train = deepLearningPrep(train_data)
  history = model.fit(X_train, y_train, epochs = 20,batch_size = 4, verbose = 1)
  return model, history
